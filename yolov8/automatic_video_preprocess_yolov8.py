# -*- coding: utf-8 -*-
"""Automatic_Video_Preprocess_YOLOv8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WM7wpspR9LB5rL8XA0xbi9KArds1zyWm
"""

# from google.colab import drive
# drive.mount('/content/drive')

!pip uninstall numpy
!pip install -U numpy==1.23.5

import numpy as np
print(np.__version__)

import os
HOME = os.getcwd()
print(HOME)

!pip install --upgrade --no-cache-dir gdown
!gdown 15AvETgieRvM1jR7LqrheKlks9u8t5D1Z
# https://drive.google.com/file/d/15AvETgieRvM1jR7LqrheKlks9u8t5D1Z/view?usp=sharing

# %cd {HOME}
# !wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-" -O vehicle-counting.mp4 && rm -rf /tmp/cookies.txt

SOURCE_VIDEO_PATH ="/content/BEAUTIFUL_British_Airways_Airbus_A350_Landing_Heathrow_RtIyFx9w1.mp4"

# Pip install method (recommended)

!pip install ultralytics

from IPython import display
display.clear_output()

import ultralytics
ultralytics.checks()

!pip install loguru

!pip install yolox

!pip install thop

!pip install lap

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}
!git clone https://github.com/ifzhang/ByteTrack.git
# %cd {HOME}/ByteTrack

# workaround related to https://github.com/roboflow/notebooks/issues/80
!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt

!pip3 install -q -r requirements.txt
!python3 setup.py -q develop
!pip install -q cython_bbox
!pip install -q onemetric

from IPython import display
display.clear_output()


import sys
sys.path.append(f"{HOME}/ByteTrack")


import yolox
print("yolox.__version__:", yolox.__version__)

from yolox.tracker.byte_tracker import BYTETracker, STrack
from onemetric.cv.utils.iou import box_iou_batch
from dataclasses import dataclass


@dataclass(frozen=True)
class BYTETrackerArgs:
    track_thresh: float = 0.25
    track_buffer: int = 30
    match_thresh: float = 0.8
    aspect_ratio_thresh: float = 3.0
    min_box_area: float = 1.0
    mot20: bool = False

!pip install supervision==0.1.0


from IPython import display
display.clear_output()


import supervision
print("supervision.__version__:", supervision.__version__)

from supervision.draw.color import ColorPalette
from supervision.geometry.dataclasses import Point
from supervision.video.dataclasses import VideoInfo
from supervision.video.source import get_video_frames_generator
from supervision.video.sink import VideoSink
from supervision.notebook.utils import show_frame_in_notebook
from supervision.tools.detections import Detections, BoxAnnotator
from supervision.tools.line_counter import LineCounter, LineCounterAnnotator

from typing import List

import numpy as np


# converts Detections into format that can be consumed by match_detections_with_tracks function
def detections2boxes(detections: Detections) -> np.ndarray:
    return np.hstack((
        detections.xyxy,
        detections.confidence[:, np.newaxis]
    ))


# converts List[STrack] into format that can be consumed by match_detections_with_tracks function
def tracks2boxes(tracks: List[STrack]) -> np.ndarray:
    return np.array([
        track.tlbr
        for track
        in tracks
    ], dtype=float)


# matches our bounding boxes with predictions
def match_detections_with_tracks(
    detections: Detections,
    tracks: List[STrack]
) -> Detections:
    if not np.any(detections.xyxy) or len(tracks) == 0:
        return np.empty((0,))

    tracks_boxes = tracks2boxes(tracks=tracks)
    iou = box_iou_batch(tracks_boxes, detections.xyxy)
    track2detection = np.argmax(iou, axis=1)

    tracker_ids = [None] * len(detections)

    for tracker_index, detection_index in enumerate(track2detection):
        if iou[tracker_index, detection_index] != 0:
            tracker_ids[detection_index] = tracks[tracker_index].track_id

    return tracker_ids

"""## Load pre-trained YOLOv8 model"""

# settings
#MODEL = "/content/drive/MyDrive/best.pt"

MODEL="yolov8m.pt"

from ultralytics import YOLO

model = YOLO(MODEL)
model.fuse()

# dict maping class_id to class_name
CLASS_NAMES_DICT = {4: 'airplane'}
# class_ids of interest - car, motorcycle, bus and truck
CLASS_ID = [4]

model.names

# Commented out IPython magic to ensure Python compatibility.
# create frame generator
generator = get_video_frames_generator(SOURCE_VIDEO_PATH)
# create instance of BoxAnnotator
box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)
# acquire first video frame
iterator = iter(generator)
frame = next(iterator)
# model prediction on single frame and conversion to supervision Detections
results = model(frame)
detections = Detections(
    xyxy=results[0].boxes.xyxy.cpu().numpy(),
    confidence=results[0].boxes.conf.cpu().numpy(),
    class_id=results[0].boxes.cls.cpu().numpy().astype(int)
)
# format custom labels
labels = [
    f"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}"
    for _, confidence, class_id, tracker_id
    in detections
]
# annotate and display frame
frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)

# %matplotlib inline
show_frame_in_notebook(frame, (16, 16))

# settings
LINE_START = Point(50, 150)
LINE_END = Point(320, 150)

TARGET_VIDEO_PATH = f"{HOME}/result.mp4"

VideoInfo.from_video_path(SOURCE_VIDEO_PATH)

"""# Find BBox (Track)"""

import cv2
from tqdm.notebook import tqdm

# create BYTETracker instance
byte_tracker = BYTETracker(BYTETrackerArgs())
# create VideoInfo instance
video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
# create frame generator
generator = get_video_frames_generator(SOURCE_VIDEO_PATH)

# open target video file
with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
    # loop over video frames
    for frame in tqdm(generator, total=video_info.total_frames):
        # model prediction on single frame and conversion to supervision Detections
        results = model(frame)
        detections = Detections(
            xyxy=results[0].boxes.xyxy.cpu().numpy(),
            confidence=results[0].boxes.conf.cpu().numpy(),
            class_id=results[0].boxes.cls.cpu().numpy().astype(int)
        )

        # tracking detections
        tracks = byte_tracker.update(
            output_results=detections2boxes(detections=detections),
            img_info=frame.shape,
            img_size=frame.shape
        )

        # Draw bounding boxes and write coordinates on each frame
        for track in tracks:
            track_id = track.track_id
            coordinates = track.tlwh
            print(f"Track ID: {track_id}, Coordinates: {coordinates}")


            # Draw the bounding box
            bbox_color = (0, 255, 0)  # Green color
            bbox_thickness = 2
            cv2.rectangle(frame, (int(coordinates[0]), int(coordinates[1])),
                          (int(coordinates[0] + coordinates[2]), int(coordinates[1] + coordinates[3])),
                          bbox_color, bbox_thickness)

            # Write the coordinates on the frame
            text = f"{coordinates}"
            text_color = (0, 0, 255)  # Red color
            text_scale = 0.6
            text_thickness = 1
            font = cv2.FONT_HERSHEY_SIMPLEX
            text_x = int(coordinates[0])
            text_y = int(coordinates[1]) - 10 if int(coordinates[1]) >= 20 else int(coordinates[1]) + 20

            cv2.putText(frame, text, (text_x, text_y), font, text_scale, text_color, text_thickness, cv2.LINE_AA)

        # Write the annotated frame to the target video file
        sink.write_frame(frame)

# def calculate_center(x, y, w, h):
#     center_x = x + w // 2
#     center_y = y + h // 2
#     return center_x, center_y

"""# Make Other Pixels Black"""

import cv2
from tqdm.notebook import tqdm

# Create BYTETracker instance
byte_tracker = BYTETracker(BYTETrackerArgs())
# Create VideoInfo instance
video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
# Create frame generator
generator = get_video_frames_generator(SOURCE_VIDEO_PATH)

# Open target video file
with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
    # Loop over video frames
    for frame in tqdm(generator, total=video_info.total_frames):
        # Model prediction on single frame and conversion to supervision Detections
        results = model(frame)
        detections = Detections(
            xyxy=results[0].boxes.xyxy.cpu().numpy(),
            confidence=results[0].boxes.conf.cpu().numpy(),
            class_id=results[0].boxes.cls.cpu().numpy().astype(int)
        )

        # Tracking detections
        tracks = byte_tracker.update(
            output_results=detections2boxes(detections=detections),
            img_info=frame.shape,
            img_size=frame.shape
        )

        # Create a mask to darken outside the bounding boxes
        mask = np.zeros_like(frame)

        # Draw bounding boxes and write coordinates on each frame
        for track in tracks:
            coordinates = track.tlwh
            margin_w = int(0.1 * coordinates[2])
            margin_h = int(0.1 * coordinates[3])
            x1 = max(0, int(coordinates[0]) - margin_w)
            y1 = max(0, int(coordinates[1]) - margin_h)
            x2 = min(frame.shape[1], int(coordinates[0] + coordinates[2] + margin_w))
            y2 = min(frame.shape[0], int(coordinates[1] + coordinates[3] + margin_h))

            # Define bounding box for drawing and masking
            cv2.rectangle(mask, (x1, y1), (x2, y2), (255, 255, 255), thickness=-1)

        # Apply the mask to keep only the regions inside the extended bounding boxes
        frame = cv2.bitwise_and(frame, mask)

        # Write the modified frame to the target video file
        sink.write_frame(frame)

"""`See /content/result.mp4 File!`"""